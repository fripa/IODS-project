---
title: "chapter4"
author: "Frida Gyllenberg"
date: "29 Nov 2017"
output: html_document
---

```{r setup 1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#libraries
library(MASS)
library(tidyverse)
library(ggplot2)
library(GGally)
library(corrplot)
```

## The data
The data set I will use for this exercise is a data set on  housing values in the surburbs of Boston. There is 506 observations of the following 14 variables: 

CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per $10,000
PTRATIO - pupil-teacher ratio by town
BLACK - 1000(Bk - 0.63)^2 where Bk isthe proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in $1000's

```{r }
getwd()
dim(Boston)
glimpse(Boston)
colnames(Boston)
```

## summary of the variables
Here is the summary of all the variables. THere is large variance on crime rates, with a fem locations with high crime that drives the mean up, but the median crime rate per capita is much lower (mean = 3.6, median=0.3). THe same is seen on the land areal variable zn. 

```{r}
summary(Boston)
```
## Graphical overview of the data
First, with the pairs function I display pairwise scatterplots of the predictors in the data set. As this is difficult to read I continue with boxplots of a few chosen variables.

```{r , echo=FALSE}
pairs(Boston)

par(mfrow=c(2,2))
hist(Boston$crim, main="Crime Rates",breaks="FD")
hist(Boston$rm, main="Main number of rooms", breaks="FD")
hist(Boston$tax, main="Tax rates", breaks="FD")
hist(Boston$ptratio, main="Pupil-teacher ratio", breaks="FD")

```
##Graphical Overview over correlations within the data

A corrplot of the variables in the data set is a nice way to get information on how  the variables of the data are correlated. (Positive correlations are displayed in blue and negative correlations in red color, (Color intensity and the size of the circle are proportional to the correlation coefficients.)

```{r , echo=FALSE}
M <- cor(Boston)
corrplot(M, method = "circle")
```
## Standardising the data set
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```
The scale() function subtracts the column means from the corresponding columns and divides the difference with standard deviation. Hence, all means are 0 in teh scaled dataset. 

##Categorical crime rate variable. 
```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# creating a quantile vector of crim 
bins <- quantile(boston_scaled$crim)
bins

# creating a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks=bins, include.lowest = TRUE, label= c("low", "med_low", "med_high", "high"))

table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

## Dividing the dataset to train and test sets

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set with only those 80% random rows
train <- boston_scaled[ind,]

# create test set excluding those rows in the train set
test <- boston_scaled[-ind,]

```

## LDA analysis on the train set and drawing the plot
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)

```
#Prediction
```{r}
#saving the crime categories from the test set
crime.correct <- test$crime

#remove crime variable from test dataset

test2 <- dplyr::select(test, -crime)
colnames(test) # with crime
test$crime #categorical
colnames(test2) # without crime

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = crime.correct, predicted = lda.pred$class)

```
The prediction works well, especially on med_high and high crime rates. 

# Reloading, standardizing and plotting again
This part was not clear to me, wonder if I got it right...
```{r}
library(MASS)
View(Boston)
#standardize the data set
Boston_scaled2 <- scale(Boston)

# calculating the euclidean distance between obesrvations, the most common distance measure
dist_eu <- dist(Boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <-kmeans(Boston, centers = 3)

# trying different numbers of clusters (1:3)
km <-kmeans(Boston, centers = 3)
summary(km)

# plot the Boston dataset with clusters
pairs(Boston, col=km$cluster)

```